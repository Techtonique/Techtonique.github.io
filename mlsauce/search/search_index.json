{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mlsauce | Star Welcome to mlsauce 's website. mlsauce contains Miscellaneous Statistical/Machine learning stuff. Its source code is available on GitHub . You can read posts about mlsauce in this blog , and for current references, feel free consult the section: References . Looking for a specific function? You can also use the search function available in the navigation bar. Installing Python 1st method : by using pip at the command line for the stable version pip install mlsauce 2nd method : from Github, for the development version pip install git+https://github.com/Techtonique/mlsauce.git R 1st method : From Github, in R console: library(devtools) devtools::install_github(\"Techtonique/mlsauce/R-package\") library(mlsauce) Quickstart AdaOpt: probabilistic classifier based on a mix of multivariable optimization and a nearest neighbors algorithm LSBoost: Gradient Boosted randomized networks using Least Squares Plus examples of use: For classification For regression Some R examples can be found in this post: https://thierrymoudiki.github.io/blog/#LSBoost Documentation The documentation for each model can be found (work in progress) here: For classifiers For regressors Contributing Want to contribute to mlsauce 's development on Github, read this !","title":"mlsauce | <a class=\"github-button\" href=\"https://github.com/Techtonique/mlsauce/stargazers\" data-color-scheme=\"no-preference: light; light: light; dark: dark;\" data-size=\"large\" aria-label=\"Star dask/dask on GitHub\">Star</a>"},{"location":"#mlsauce-star","text":"Welcome to mlsauce 's website. mlsauce contains Miscellaneous Statistical/Machine learning stuff. Its source code is available on GitHub . You can read posts about mlsauce in this blog , and for current references, feel free consult the section: References . Looking for a specific function? You can also use the search function available in the navigation bar.","title":"mlsauce | Star"},{"location":"#installing","text":"","title":"Installing"},{"location":"#python","text":"1st method : by using pip at the command line for the stable version pip install mlsauce 2nd method : from Github, for the development version pip install git+https://github.com/Techtonique/mlsauce.git","title":"Python"},{"location":"#r","text":"1st method : From Github, in R console: library(devtools) devtools::install_github(\"Techtonique/mlsauce/R-package\") library(mlsauce)","title":"R"},{"location":"#quickstart","text":"AdaOpt: probabilistic classifier based on a mix of multivariable optimization and a nearest neighbors algorithm LSBoost: Gradient Boosted randomized networks using Least Squares Plus examples of use: For classification For regression Some R examples can be found in this post: https://thierrymoudiki.github.io/blog/#LSBoost","title":"Quickstart"},{"location":"#documentation","text":"The documentation for each model can be found (work in progress) here: For classifiers For regressors","title":"Documentation"},{"location":"#contributing","text":"Want to contribute to mlsauce 's development on Github, read this !","title":"Contributing"},{"location":"CONTRIBUTING/","text":"mlsauce Code of Conduct 1. Purpose A primary goal of this project is to be inclusive to the largest number of contributors, and most importantly with the most varied and diverse backgrounds possible . As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion, or lack of religion thereof. This code of conduct outlines our expectations for all those who participate to the project, as well as the consequences for unacceptable behavior. We invite all those who participate in, to help us create safe and positive experiences for everyone. 2. Open [Source/Culture/Tech] Citizenship A supplemental goal of this Code of Conduct is to encourage participants to recognize and strengthen the relationships between our actions and their effects on other participants. Communities mirror the societies in which they exist, and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society. 3. Expected Behavior The following behaviors are expected and requested of all contributors: Attempt collaboration before conflict . Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this project. Exercise consideration and respect in your speech and actions. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants. 4. Unacceptable Behavior The following behaviors are considered harassment and are unacceptable: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people's personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Unwelcome sexual attention. This includes, sexualized comments or jokes. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior. 5. Consequences of Unacceptable Behavior Unacceptable behavior from any contributor will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a contributor engages in unacceptable behavior, appropriate action will be taken, up to and including a temporary ban or permanent expulsion without warning. 6. Scope We expect all contributors to abide by this Code of Conduct in all venues, online and in-person. 7. Contact info thierry.moudiki AT gmail.com 8. License and attribution Portions of text derived from the Citizen Code of Conduct .","title":"Contributing"},{"location":"CONTRIBUTING/#mlsauce-code-of-conduct","text":"","title":"mlsauce Code of Conduct"},{"location":"CONTRIBUTING/#1-purpose","text":"A primary goal of this project is to be inclusive to the largest number of contributors, and most importantly with the most varied and diverse backgrounds possible . As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion, or lack of religion thereof. This code of conduct outlines our expectations for all those who participate to the project, as well as the consequences for unacceptable behavior. We invite all those who participate in, to help us create safe and positive experiences for everyone.","title":"1. Purpose"},{"location":"CONTRIBUTING/#2-open-sourceculturetech-citizenship","text":"A supplemental goal of this Code of Conduct is to encourage participants to recognize and strengthen the relationships between our actions and their effects on other participants. Communities mirror the societies in which they exist, and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society.","title":"2. Open [Source/Culture/Tech] Citizenship"},{"location":"CONTRIBUTING/#3-expected-behavior","text":"The following behaviors are expected and requested of all contributors: Attempt collaboration before conflict . Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this project. Exercise consideration and respect in your speech and actions. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants.","title":"3. Expected Behavior"},{"location":"CONTRIBUTING/#4-unacceptable-behavior","text":"The following behaviors are considered harassment and are unacceptable: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people's personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Unwelcome sexual attention. This includes, sexualized comments or jokes. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior.","title":"4. Unacceptable Behavior"},{"location":"CONTRIBUTING/#5-consequences-of-unacceptable-behavior","text":"Unacceptable behavior from any contributor will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a contributor engages in unacceptable behavior, appropriate action will be taken, up to and including a temporary ban or permanent expulsion without warning.","title":"5. Consequences of Unacceptable Behavior"},{"location":"CONTRIBUTING/#6-scope","text":"We expect all contributors to abide by this Code of Conduct in all venues, online and in-person.","title":"6. Scope"},{"location":"CONTRIBUTING/#7-contact-info","text":"thierry.moudiki AT gmail.com","title":"7. Contact info"},{"location":"CONTRIBUTING/#8-license-and-attribution","text":"Portions of text derived from the Citizen Code of Conduct .","title":"8. License and attribution"},{"location":"LICENSE/","text":"The Clear BSD License Copyright (c) [2019] [T. Moudiki] All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"REFERENCES/","text":"Current references - Moudiki, T. (2020). AdaOpt: Multivariable optimization for classification. Available at: https://www.researchgate.net/publication/341409169_AdaOpt_Multivariable_optimization_for_classification","title":"References"},{"location":"REFERENCES/#current-references","text":"- Moudiki, T. (2020). AdaOpt: Multivariable optimization for classification. Available at: https://www.researchgate.net/publication/341409169_AdaOpt_Multivariable_optimization_for_classification","title":"Current references"},{"location":"documentation/classifiers/","text":"Classifiers In alphabetical order [source] AdaOpt mlsauce.AdaOpt( n_iterations=50, learning_rate=0.3, reg_lambda=0.1, reg_alpha=0.5, eta=0.01, gamma=0.01, k=3, tolerance=0, n_clusters=0, batch_size=100, row_sample=0.8, type_dist=\"euclidean-f\", n_jobs=None, verbose=0, cache=True, seed=123, ) AdaOpt classifier. Attributes: n_iterations: int number of iterations of the optimizer at training time. learning_rate: float controls the speed of the optimizer at training time. reg_lambda: float L2 regularization parameter for successive errors in the optimizer (at training time). reg_alpha: float L1 regularization parameter for successive errors in the optimizer (at training time). eta: float controls the slope in gradient descent (at training time). gamma: float controls the step size in gradient descent (at training time). k: int number of nearest neighbors selected at test time for classification. tolerance: float controls early stopping in gradient descent (at training time). n_clusters: int number of clusters, if MiniBatch k-means is used at test time (for faster prediction). batch_size: int size of the batch, if MiniBatch k-means is used at test time (for faster prediction). row_sample: float percentage of rows chosen from training set (by stratified subsampling, for faster prediction). type_dist: str distance used for finding the nearest neighbors; currently `euclidean-f` (euclidean distances calculated as whole), `euclidean` (euclidean distances calculated row by row), `cosine` (cosine distance). n_jobs: int number of cpus for parallel processing (default: None) verbose: int progress bar for parallel processing (yes = 1) or not (no = 0) cache: boolean if the nearest neighbors are cached or not, for faster retrieval in subsequent calls. seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout. [source] fit AdaOpt.fit(X, y, **kwargs) Fit AdaOpt to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source] predict AdaOpt.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source] predict_proba AdaOpt.predict_proba(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source] LSBoostClassifier mlsauce.LSBoostClassifier( n_estimators=100, learning_rate=0.1, n_hidden_features=5, reg_lambda=0.1, row_sample=1, col_sample=1, dropout=0, tolerance=0.0001, direct_link=1, verbose=1, seed=123, backend=\"cpu\", solver=\"ridge\", ) LSBoost classifier. Attributes: n_estimators: int number of boosting iterations. learning_rate: float controls the learning speed at training time. n_hidden_features: int number of nodes in successive hidden layers. reg_lambda: float L2 regularization parameter for successive errors in the optimizer (at training time). row_sample: float percentage of rows chosen from the training set. col_sample: float percentage of columns chosen from the training set. dropout: float percentage of nodes dropped from the training set. tolerance: float controls early stopping in gradient descent (at training time). direct_link: bool indicates whether the original features are included (True) in model's fitting or not (False). verbose: int progress bar (yes = 1) or not (no = 0) (currently). seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu') solver: str type of 'weak' learner; currently in ('ridge', 'lasso') [source] fit LSBoostClassifier.fit(X, y, **kwargs) Fit Booster (classifier) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source] predict LSBoostClassifier.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source] predict_proba LSBoostClassifier.predict_proba(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source] StumpClassifier mlsauce.StumpClassifier(bins=\"auto\") Stump classifier. Attributes: bins: int Number of histogram bins; as in numpy.histogram. [source] fit StumpClassifier.fit(X, y, sample_weight=None, **kwargs) Fit Stump to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. sample_weight: array_like, shape = [n_samples] Observations weights. Returns: self: object. [source] predict StumpClassifier.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source] predict_proba StumpClassifier.predict_proba(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like}","title":"Classifiers"},{"location":"documentation/classifiers/#classifiers","text":"In alphabetical order [source]","title":"Classifiers"},{"location":"documentation/classifiers/#adaopt","text":"mlsauce.AdaOpt( n_iterations=50, learning_rate=0.3, reg_lambda=0.1, reg_alpha=0.5, eta=0.01, gamma=0.01, k=3, tolerance=0, n_clusters=0, batch_size=100, row_sample=0.8, type_dist=\"euclidean-f\", n_jobs=None, verbose=0, cache=True, seed=123, ) AdaOpt classifier. Attributes: n_iterations: int number of iterations of the optimizer at training time. learning_rate: float controls the speed of the optimizer at training time. reg_lambda: float L2 regularization parameter for successive errors in the optimizer (at training time). reg_alpha: float L1 regularization parameter for successive errors in the optimizer (at training time). eta: float controls the slope in gradient descent (at training time). gamma: float controls the step size in gradient descent (at training time). k: int number of nearest neighbors selected at test time for classification. tolerance: float controls early stopping in gradient descent (at training time). n_clusters: int number of clusters, if MiniBatch k-means is used at test time (for faster prediction). batch_size: int size of the batch, if MiniBatch k-means is used at test time (for faster prediction). row_sample: float percentage of rows chosen from training set (by stratified subsampling, for faster prediction). type_dist: str distance used for finding the nearest neighbors; currently `euclidean-f` (euclidean distances calculated as whole), `euclidean` (euclidean distances calculated row by row), `cosine` (cosine distance). n_jobs: int number of cpus for parallel processing (default: None) verbose: int progress bar for parallel processing (yes = 1) or not (no = 0) cache: boolean if the nearest neighbors are cached or not, for faster retrieval in subsequent calls. seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout. [source]","title":"AdaOpt"},{"location":"documentation/classifiers/#fit","text":"AdaOpt.fit(X, y, **kwargs) Fit AdaOpt to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source]","title":"fit"},{"location":"documentation/classifiers/#predict","text":"AdaOpt.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source]","title":"predict"},{"location":"documentation/classifiers/#predict_proba","text":"AdaOpt.predict_proba(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source]","title":"predict_proba"},{"location":"documentation/classifiers/#lsboostclassifier","text":"mlsauce.LSBoostClassifier( n_estimators=100, learning_rate=0.1, n_hidden_features=5, reg_lambda=0.1, row_sample=1, col_sample=1, dropout=0, tolerance=0.0001, direct_link=1, verbose=1, seed=123, backend=\"cpu\", solver=\"ridge\", ) LSBoost classifier. Attributes: n_estimators: int number of boosting iterations. learning_rate: float controls the learning speed at training time. n_hidden_features: int number of nodes in successive hidden layers. reg_lambda: float L2 regularization parameter for successive errors in the optimizer (at training time). row_sample: float percentage of rows chosen from the training set. col_sample: float percentage of columns chosen from the training set. dropout: float percentage of nodes dropped from the training set. tolerance: float controls early stopping in gradient descent (at training time). direct_link: bool indicates whether the original features are included (True) in model's fitting or not (False). verbose: int progress bar (yes = 1) or not (no = 0) (currently). seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu') solver: str type of 'weak' learner; currently in ('ridge', 'lasso') [source]","title":"LSBoostClassifier"},{"location":"documentation/classifiers/#fit_1","text":"LSBoostClassifier.fit(X, y, **kwargs) Fit Booster (classifier) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source]","title":"fit"},{"location":"documentation/classifiers/#predict_1","text":"LSBoostClassifier.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source]","title":"predict"},{"location":"documentation/classifiers/#predict_proba_1","text":"LSBoostClassifier.predict_proba(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source]","title":"predict_proba"},{"location":"documentation/classifiers/#stumpclassifier","text":"mlsauce.StumpClassifier(bins=\"auto\") Stump classifier. Attributes: bins: int Number of histogram bins; as in numpy.histogram. [source]","title":"StumpClassifier"},{"location":"documentation/classifiers/#fit_2","text":"StumpClassifier.fit(X, y, sample_weight=None, **kwargs) Fit Stump to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. sample_weight: array_like, shape = [n_samples] Observations weights. Returns: self: object. [source]","title":"fit"},{"location":"documentation/classifiers/#predict_2","text":"StumpClassifier.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source]","title":"predict"},{"location":"documentation/classifiers/#predict_proba_2","text":"StumpClassifier.predict_proba(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like}","title":"predict_proba"},{"location":"documentation/regressors/","text":"Regressors In alphabetical order [source] LassoRegressor mlsauce.LassoRegressor(reg_lambda=0.1, max_iter=10, tol=0.001, backend=\"cpu\") Lasso. Attributes: reg_lambda: float L1 regularization parameter. max_iter: int number of iterations of lasso shooting algorithm. tol: float tolerance for convergence of lasso shooting algorithm. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu'). [source] fit LassoRegressor.fit(X, y, **kwargs) Fit matrixops (classifier) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source] predict LassoRegressor.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source] LSBoostRegressor mlsauce.LSBoostRegressor( n_estimators=100, learning_rate=0.1, n_hidden_features=5, reg_lambda=0.1, row_sample=1, col_sample=1, dropout=0, tolerance=0.0001, direct_link=1, verbose=1, seed=123, backend=\"cpu\", solver=\"ridge\", ) LSBoost regressor. Attributes: n_estimators: int number of boosting iterations. learning_rate: float controls the learning speed at training time. n_hidden_features: int number of nodes in successive hidden layers. reg_lambda: float L2 regularization parameter for successive errors in the optimizer (at training time). row_sample: float percentage of rows chosen from the training set. col_sample: float percentage of columns chosen from the training set. dropout: float percentage of nodes dropped from the training set. tolerance: float controls early stopping in gradient descent (at training time). direct_link: bool indicates whether the original features are included (True) in model's fitting or not (False). verbose: int progress bar (yes = 1) or not (no = 0) (currently). seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu') solver: str type of 'weak' learner; currently in ('ridge', 'lasso') [source] fit LSBoostRegressor.fit(X, y, **kwargs) Fit Booster (regressor) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source] predict LSBoostRegressor.predict(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source] RidgeRegressor mlsauce.RidgeRegressor(reg_lambda=0.1, backend=\"cpu\") Ridge. Attributes: reg_lambda: float regularization parameter. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu') [source] fit RidgeRegressor.fit(X, y, **kwargs) Fit matrixops (classifier) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source] predict RidgeRegressor.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like}","title":"Regressors"},{"location":"documentation/regressors/#regressors","text":"In alphabetical order [source]","title":"Regressors"},{"location":"documentation/regressors/#lassoregressor","text":"mlsauce.LassoRegressor(reg_lambda=0.1, max_iter=10, tol=0.001, backend=\"cpu\") Lasso. Attributes: reg_lambda: float L1 regularization parameter. max_iter: int number of iterations of lasso shooting algorithm. tol: float tolerance for convergence of lasso shooting algorithm. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu'). [source]","title":"LassoRegressor"},{"location":"documentation/regressors/#fit","text":"LassoRegressor.fit(X, y, **kwargs) Fit matrixops (classifier) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source]","title":"fit"},{"location":"documentation/regressors/#predict","text":"LassoRegressor.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like} [source]","title":"predict"},{"location":"documentation/regressors/#lsboostregressor","text":"mlsauce.LSBoostRegressor( n_estimators=100, learning_rate=0.1, n_hidden_features=5, reg_lambda=0.1, row_sample=1, col_sample=1, dropout=0, tolerance=0.0001, direct_link=1, verbose=1, seed=123, backend=\"cpu\", solver=\"ridge\", ) LSBoost regressor. Attributes: n_estimators: int number of boosting iterations. learning_rate: float controls the learning speed at training time. n_hidden_features: int number of nodes in successive hidden layers. reg_lambda: float L2 regularization parameter for successive errors in the optimizer (at training time). row_sample: float percentage of rows chosen from the training set. col_sample: float percentage of columns chosen from the training set. dropout: float percentage of nodes dropped from the training set. tolerance: float controls early stopping in gradient descent (at training time). direct_link: bool indicates whether the original features are included (True) in model's fitting or not (False). verbose: int progress bar (yes = 1) or not (no = 0) (currently). seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu') solver: str type of 'weak' learner; currently in ('ridge', 'lasso') [source]","title":"LSBoostRegressor"},{"location":"documentation/regressors/#fit_1","text":"LSBoostRegressor.fit(X, y, **kwargs) Fit Booster (regressor) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source]","title":"fit"},{"location":"documentation/regressors/#predict_1","text":"LSBoostRegressor.predict(X, **kwargs) Predict probabilities for test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source]","title":"predict"},{"location":"documentation/regressors/#ridgeregressor","text":"mlsauce.RidgeRegressor(reg_lambda=0.1, backend=\"cpu\") Ridge. Attributes: reg_lambda: float regularization parameter. backend: str type of backend; must be in ('cpu', 'gpu', 'tpu') [source]","title":"RidgeRegressor"},{"location":"documentation/regressors/#fit_2","text":"RidgeRegressor.fit(X, y, **kwargs) Fit matrixops (classifier) to training data (X, y) Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set. Returns: self: object. [source]","title":"fit"},{"location":"documentation/regressors/#predict_2","text":"RidgeRegressor.predict(X, **kwargs) Predict test data X. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to `predict_proba` Returns: model predictions: {array-like}","title":"predict"},{"location":"documentation/scoring_metrics/","text":"Scoring metrics Regression metrics explained_variance : Explained variance regression score function mean_absolute_error : Mean absolute error regression loss mean_squared_error : Mean squared error regression loss mean_squared_log_error : Mean squared logarithmic error regression loss median_absolute_error : Median absolute error regression loss r2 : R^2 (coefficient of determination) regression score function. Classification metrics accuracy : Accuracy classification score. average_precision : Compute average precision (AP) from prediction scores brier_score_loss : Compute the Brier score. f1_score : Compute the F1 score, also known as balanced F-score or F-measure neg_log_loss : Negative Log loss, aka logistic loss or cross-entropy loss. precision : Compute the precision recall : Compute the recall roc_auc : Compute Area Under the Curve (AUC) using the trapezoidal rule","title":"Scoring metrics"},{"location":"documentation/scoring_metrics/#scoring-metrics","text":"","title":"Scoring metrics"},{"location":"documentation/scoring_metrics/#regression-metrics","text":"explained_variance : Explained variance regression score function mean_absolute_error : Mean absolute error regression loss mean_squared_error : Mean squared error regression loss mean_squared_log_error : Mean squared logarithmic error regression loss median_absolute_error : Median absolute error regression loss r2 : R^2 (coefficient of determination) regression score function.","title":"Regression metrics"},{"location":"documentation/scoring_metrics/#classification-metrics","text":"accuracy : Accuracy classification score. average_precision : Compute average precision (AP) from prediction scores brier_score_loss : Compute the Brier score. f1_score : Compute the F1 score, also known as balanced F-score or F-measure neg_log_loss : Negative Log loss, aka logistic loss or cross-entropy loss. precision : Compute the precision recall : Compute the recall roc_auc : Compute Area Under the Curve (AUC) using the trapezoidal rule","title":"Classification metrics"},{"location":"examples/classification/","text":"Classification example Other examples can be found here: https://thierrymoudiki.github.io/blog/#mlsauce and in this repo . import numpy as np from sklearn.datasets import load_digits, load_breast_cancer, load_wine, load_iris from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from time import time from os import chdir from sklearn import metrics import mlsauce as ms # data 1 breast_cancer = load_breast_cancer() X = breast_cancer.data y = breast_cancer.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.AdaOpt(n_jobs=4, type_dist=\"euclidean\", verbose=1) #obj = ms.AdaOpt() start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # data 2 wine = load_wine() Z = wine.data t = wine.target np.random.seed(879423) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) obj = ms.AdaOpt() start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # data 3 iris = load_iris() Z = iris.data t = iris.target np.random.seed(734563) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) obj = ms.AdaOpt() start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # data 4 digits = load_digits() Z = digits.data t = digits.target np.random.seed(13239) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) obj = ms.AdaOpt(n_iterations=50, learning_rate=0.3, reg_lambda=0.1, reg_alpha=0.5, eta=0.01, gamma=0.01, tolerance=1e-4, row_sample=1, k=1, n_jobs=3, type_dist=\"euclidean\", verbose=1) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # with clustering obj = ms.AdaOpt(n_clusters=25, k=1) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start)","title":"Classification"},{"location":"examples/classification/#classification-example","text":"Other examples can be found here: https://thierrymoudiki.github.io/blog/#mlsauce and in this repo . import numpy as np from sklearn.datasets import load_digits, load_breast_cancer, load_wine, load_iris from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from time import time from os import chdir from sklearn import metrics import mlsauce as ms # data 1 breast_cancer = load_breast_cancer() X = breast_cancer.data y = breast_cancer.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.AdaOpt(n_jobs=4, type_dist=\"euclidean\", verbose=1) #obj = ms.AdaOpt() start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # data 2 wine = load_wine() Z = wine.data t = wine.target np.random.seed(879423) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) obj = ms.AdaOpt() start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # data 3 iris = load_iris() Z = iris.data t = iris.target np.random.seed(734563) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) obj = ms.AdaOpt() start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # data 4 digits = load_digits() Z = digits.data t = digits.target np.random.seed(13239) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) obj = ms.AdaOpt(n_iterations=50, learning_rate=0.3, reg_lambda=0.1, reg_alpha=0.5, eta=0.01, gamma=0.01, tolerance=1e-4, row_sample=1, k=1, n_jobs=3, type_dist=\"euclidean\", verbose=1) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start) # with clustering obj = ms.AdaOpt(n_clusters=25, k=1) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(obj.score(X_test, y_test)) print(time()-start)","title":"Classification example"},{"location":"examples/regression/","text":"Regression example Other examples can be found here: https://thierrymoudiki.github.io/blog/#mlsauce and in this repo . import numpy as np from sklearn.datasets import load_boston, load_diabetes from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from time import time from os import chdir from sklearn import metrics import mlsauce as ms # ridge print(\"\\n\") print(\"ridge -----\") print(\"\\n\") # data 1 boston = load_boston() X = boston.data y = boston.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor() print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) print(obj) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # MORE DATA NEEDED # MORE DATA NEEDED # MORE DATA NEEDED obj = ms.LSBoostRegressor(backend=\"gpu\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) print(obj) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # data 2 diabetes = load_diabetes() X = diabetes.data y = diabetes.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor() print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # MORE DATA NEEDED # MORE DATA NEEDED # MORE DATA NEEDED obj = ms.LSBoostRegressor(backend=\"gpu\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # lasso print(\"\\n\") print(\"lasso -----\") print(\"\\n\") # data 1 boston = load_boston() X = boston.data y = boston.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor(solver=\"lasso\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) print(obj) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # data 2 diabetes = load_diabetes() X = diabetes.data y = diabetes.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor(solver=\"lasso\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start)","title":"Regression"},{"location":"examples/regression/#regression-example","text":"Other examples can be found here: https://thierrymoudiki.github.io/blog/#mlsauce and in this repo . import numpy as np from sklearn.datasets import load_boston, load_diabetes from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score from time import time from os import chdir from sklearn import metrics import mlsauce as ms # ridge print(\"\\n\") print(\"ridge -----\") print(\"\\n\") # data 1 boston = load_boston() X = boston.data y = boston.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor() print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) print(obj) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # MORE DATA NEEDED # MORE DATA NEEDED # MORE DATA NEEDED obj = ms.LSBoostRegressor(backend=\"gpu\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) print(obj) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # data 2 diabetes = load_diabetes() X = diabetes.data y = diabetes.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor() print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # MORE DATA NEEDED # MORE DATA NEEDED # MORE DATA NEEDED obj = ms.LSBoostRegressor(backend=\"gpu\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # lasso print(\"\\n\") print(\"lasso -----\") print(\"\\n\") # data 1 boston = load_boston() X = boston.data y = boston.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor(solver=\"lasso\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) print(obj) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start) # data 2 diabetes = load_diabetes() X = diabetes.data y = diabetes.target # split data into training test and test set np.random.seed(15029) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) obj = ms.LSBoostRegressor(solver=\"lasso\") print(obj.get_params()) start = time() obj.fit(X_train, y_train) print(time()-start) start = time() print(np.sqrt(np.mean(np.square(obj.predict(X_test) - y_test)))) print(time()-start)","title":"Regression example"}]}